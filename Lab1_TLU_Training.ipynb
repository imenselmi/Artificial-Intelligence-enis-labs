{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1 - TLU Training.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vztx58frD0wu"
      },
      "source": [
        "# The (Rosenblatt) Perceptron\n",
        "\n",
        "This version of the Perceptron should be found [here](https://colab.research.google.com/drive/1VfBv9H9Se-30OUMFfjEaq9BL2rKKo-Gy).To see a compact version of the Perceptron, check out [TinyPerceptron](https://colab.research.google.com/drive/1LmTL4WFZA1nKT6K8X2h2BYHb6S-PggEy).\n",
        "\n",
        "Please send bug reports or updates to gabriele underscore fariello at harvard dot edu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ1qoiRWx3aG"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hpl9BrRx5L5"
      },
      "source": [
        "Before you go on, you may want to make sure you're comfortable with the material covered in:\n",
        "- [Colab Basics & Mounting Google Drive](https://colab.research.google.com/drive/1vCv_PGeDZBvdsMUHrj_O8wScp0uHi2WL)\n",
        "- [Loading Data & SKLearn](https://colab.research.google.com/drive/1waXfYJevdf9kyhasvUlzw6vf8RvZfv_V)\n",
        "- Ability to read Python code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8vT_Sgwd8F"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y69E-4e-whHd"
      },
      "source": [
        "The original Rosenblatt [Perceptron](https://en.wikipedia.org/wiki/Perceptron) was invented by [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) (1928-1971) in 1957 and published in 1958 journal \"Psychological Review\":\n",
        "\n",
        "    Rosenblatt, Frank. \"The perceptron: a probabilistic model for\n",
        "    information storage and organization in the brain.\" Psychological\n",
        "    review 65.6 (1958): 386.\n",
        "\n",
        "The publication itself did not contain the code used, and, as far as we know, the original code is not accessible. Lukily the methods section contains enough information to likely approximate the origical code.\n",
        "\n",
        "\"[Toward Data Science](https://towardsdatascience.com/)\" has a [nice brief article](https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53) by Sagar Sharma explaining the Perceptron further. In case you forgot, the original Perceptron is a *Binary Linear Classifier* which attempts to draw a line (for 2 features), a plane (for 3 features), etc. which creates a decision boundary splitting the data into two regions. Samples with features in one region are labelled one way, all others in the other way.\n",
        "\n",
        "This code is a heavily modified version, with permission, of the code originally published in \"[Python Machine Learning by Sebastian Raschka](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow-ebook/dp/B0742K7HYF)\" (From the first edition, but the I recommend the 2nd edition for those interested ISBN-13: 978-1787125933 or ISBN-10: 1787125939)\n",
        "\n",
        "Sebastian Raschka also has [a good, more in-depth explanation](https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#frank-rosenblatts-perceptron) on his site."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19xsAivxnoq"
      },
      "source": [
        "# Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DH36QV3THHI"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris['feature_names'] + ['target'])\n",
        "labels, samples = np.where(dataset.iloc[0:100, 4].values==0,-1,1), dataset.iloc[0:100, [0, 2]].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhwGHiI8a1DN"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MeKGFPy5du6"
      },
      "source": [
        "# The Perceptron Class (big)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6fXKH1hwXpI"
      },
      "source": [
        "\n",
        "Below we begin the `Perceptron` class. Remember that we split it across cells using Python's ability to add methods to class after they have been declared, but you normally don't want to do that.\n",
        "\n",
        "Typically you document the class and the `__init__` function in the docstring (the string immediately following the `class` declaration. There are several formats that are widely in use, but here we'll try to stick to the [NumPy recommendation](https://numpydoc.readthedocs.io/en/latest/format.html) if and when I get around to finishing the documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l539vq8VzAuV"
      },
      "source": [
        "## Class Declaration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qws5uVro2hW1"
      },
      "source": [
        "### Perceptron & __init__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgd7BsJPzC2R"
      },
      "source": [
        "Below is the declaration of the `Perceptron` class with embedded documentation. I've decided to implement it such that `Perceptron` object are instantiated (i.e. created) with the samples and labels already passed to it. This is not the traditional way to do this since there are times when it would be preferable to re-use the Perceptron with different inputs (e.g., the Perceptrons in a multi-layer configuration). But we're not going to do that.\n",
        "\n",
        "This way, we can quickly try different variables like changing the number of iterations during training, the values of the initial weights, and the learning rage ($\\eta$) without passing around the inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8vGY7Q95du8"
      },
      "source": [
        "class Perceptron(object):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        samples : numpy.dnarray\n",
        "            An array of arrays of features (those variables we will use to train\n",
        "            on during trainging and then use when trying to \"guess\" the species\n",
        "            of the flower). Each element of the array contains an array of\n",
        "            features for that sample. For example:\n",
        "            samples = [[petal_length_1, sepal_width_1],\n",
        "                       [petal_length_2, sepal_width_2],\n",
        "                       ...\n",
        "                       [petal_length_n, sepal_width_n],\n",
        "                       ]\n",
        "        labels : numpy.ndarray\n",
        "            An array, of the same length (same size and shape of the first\n",
        "            dimension) as the `samples` array where each element if the\n",
        "            \"correct\" label in number format for each sample. If species_1 = 0\n",
        "            and species_2 = 1 it might look like this for the above sample:\n",
        "            labels = [1,0,... 1]    \n",
        "    \"\"\"\n",
        "    def __init__(self, samples, labels):\n",
        "        # Store the samples after adding a 1.0 \"hidden\" feature to each\n",
        "        # this is the \"bias value\". Many implementations separate the\n",
        "        # bias weight (calling it just bias), but this way we treat\n",
        "        # all weights the same and simplify the code (see TinyPerceptron)\n",
        "        self.samples = np.insert(samples,0,np.ones(samples.shape[0]), axis=1) # add 1.0 to each sample\n",
        "        # Store the labels\n",
        "        self.labels = labels\n",
        "        # This is a binary classifier, so if there are not two labels, we have a problem.\n",
        "        self.unique_labels = np.unique(labels)\n",
        "        if len(self.unique_labels) != 2:\n",
        "            raise ValueError(\"We need exactly two categories/labels. We received %d.\" %(len(unique_labels)))\n",
        "        self.a_label, self.b_label = self.unique_labels[0], self.unique_labels[1]\n",
        "        # Set the threshold to halfway between the two\n",
        "        self.threshold = self.unique_labels.mean()\n",
        "        if self.samples.shape[0] != labels.shape[0]:\n",
        "            raise ValueError(\"We need there to be as many samples (%d) as there are labels (%d).\"\n",
        "                            %(self.samples.shape[0], labels.shape[0]))\n",
        "        # We want to keep track of the the weights as they progress\n",
        "        self.bias_history = []\n",
        "        self.weights_history = []\n",
        "        self.iteration_count = None\n",
        "        self.num_iterations = None\n",
        "        self.learning_rate = None\n",
        "        self.misclassifications = None\n",
        "        # Initialize the weights\n",
        "        self.initialize_weights()\n",
        "        pass\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNUaG-hX1tQ_"
      },
      "source": [
        "### set_all_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFl89fcL5dvG"
      },
      "source": [
        "Since we'll be doing the same thing many times, it's good to create a function. This convenience function takes an Numpy ndarray and sets all the weights (bias included) based on the values in the array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQkpjIUn5dvH"
      },
      "source": [
        "def set_all_weights(self,arr):\n",
        "    \"\"\"\n",
        "    Set all weights (including the bias) using teh np.ndarray arr.\n",
        "    \n",
        "    You probabl don't want to call this directly. See other methods\n",
        "    below.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    \n",
        "    arr : numpy.ndarray\n",
        "        An array of the same length as the number of features per sample plus one\n",
        "        containing all the values for all the weights.\n",
        "    \"\"\"\n",
        "    if not isinstance(arr, np.ndarray):\n",
        "        raise ValueError(\"We need an np.ndarray object as input not a %s\" %type(arr))\n",
        "        pass\n",
        "    if arr.shape[0] != self.samples.shape[1]:\n",
        "        raise ValueError(\"Array must have %d elements, it had %d (%s)\" %(\n",
        "            self.samples.shape[1],arr.shape[0], arr))\n",
        "        pass\n",
        "    self.weights = arr\n",
        "    self.weights_history = [arr.copy()]\n",
        "    return self\n",
        "\n",
        "Perceptron.set_all_weights = set_all_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jAtjg7t10ql"
      },
      "source": [
        "### set_random_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_fnEzFy5dvM"
      },
      "source": [
        "This method sets the bias weight and the weights for each feature to some random value between -1.0 and 1.0 using some of the neat conveninence tricks afforded to us through NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqSUJRC45dvN"
      },
      "source": [
        "def set_random_weights(self):\n",
        "    \"\"\"\n",
        "    Set weights to random values from -1.0 to 1.0\n",
        "    \"\"\"\n",
        "    return self.set_all_weights(np.random.rand(self.samples.shape[1]) * 2 - 1.0)\n",
        "\n",
        "Perceptron.set_random_weights = set_random_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7g1ZTL_16SE"
      },
      "source": [
        "### initialize_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3o3_wya5dvT"
      },
      "source": [
        "Initialze all the weights (bias included0 to some values depending on input.\n",
        "* If value is `None`, initialize to random values between -1 and 1.\n",
        "* If value is a NumPy ndarray, set the values to the content of the array.\n",
        "* Otherwise, assume it's a number, convert it to a float, and set all values to that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcBT7yj05dvV"
      },
      "source": [
        "def initialize_weights(self,value=None):\n",
        "    \"\"\"\n",
        "    Initialize the bias and weights.\n",
        "    \n",
        "    Paremeters\n",
        "    ----------\n",
        "    value : None or float\n",
        "        If None, all weights will be initialized randomly with values ranging from -1.0 to 1.0\n",
        "        If np.ndarry, set weights with values in the array\n",
        "        Otherwise set all weights to value\n",
        "    \"\"\"\n",
        "    if value is None:\n",
        "        return self.set_random_weights()\n",
        "    # If it's an np.ndarray, assign\n",
        "    if isinstance(value, np.ndarray):\n",
        "        return self.set_all_weights(value)\n",
        "    return self.set_all_weights(np.full(self.samples.shape[1], float(value)))\n",
        "\n",
        "Perceptron.initialize_weights = initialize_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8EF5a5s2Ac3"
      },
      "source": [
        "### get_predicted_float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nql6eXWu5dva"
      },
      "source": [
        "To get the predicted value (before thresholding) which is the result of the summation function in class:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sum_{n=0}^n x_n w_n = ( x_0 w_0 + x_1 w_1 + ... x_n w_n ) = \\begin{bmatrix}\n",
        "x_0 \\\\\n",
        "x_1 \\\\\n",
        "... \\\\\n",
        "x_n\n",
        "\\end{bmatrix}\n",
        "\\bullet\n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "... \\\\\n",
        "w_n\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "Where $x_0$ = `1.0` and $w_0$ = `self.bias` are the bias value and the bias weight respectively. Since the bias value is always 1.0, we don't need to go around adding `self.bias * 1.0` but just `self.bias`. Many implementations therefore keep $w_0$ out separately, we don't.\n",
        "\n",
        "Since this the same thing as a dot-product (the stuff at the end of the second equals sign) and this could be done using `np.dot` in stead of a loop, which is better. The `np.dot` function is implemented using fast linear algebra libraries that experience a lot of acceleration if there are available GPUs. With two features per sample, this does not matter too much, but keep that in mind.\n",
        "\n",
        "**Remember** we added a `1.0` to each sample, so we can get the results of the dot-product of BOTH the normal weights ( $x_1,\\ x_2\\ ...\\ x_n$ ) per feature AND the bais ( $w_0$ ) with one `np.dot` call since the features per sample are\n",
        "\n",
        "```python\n",
        "    sample = [1.0, feature1, feature2]\n",
        "```\n",
        "\n",
        "and the weights are\n",
        "\n",
        "```python\n",
        "    self.weights = [bias_weight, feature1_weight, feature2_weight]\n",
        "```\n",
        "\n",
        "The tiny extra work done by having to multiply by `1.0` each sample is often dwarfed by the ability to use one single `np.dot` rather than multiple operations, especially when you have many features or, like we will be doing, are retraining on the same data set over and over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhP9Ryvo5dva"
      },
      "source": [
        "def get_predicted_float(self, features):\n",
        "    \"\"\"\n",
        "    \"Predict\" the un-thresholded value and return it as a float.\n",
        "    \"\"\"\n",
        "    return np.dot(features,self.weights)\n",
        "\n",
        "Perceptron.get_predicted_float = get_predicted_float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bCq2P02Egv"
      },
      "source": [
        "### predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXaJPmV35dvc"
      },
      "source": [
        "Now you may recall from lecture that artificial neurons will generally have a summation (`get_predicted_float()`) function and a transformation function where the output is transformed usually between the values of 0 and 1 or -1 and 1 in some way, sometimes using a sigmoidal function or a rectifier. The transformation function used by the Perceptron is a simple thresholding (aka step) function:\n",
        "\n",
        "\\begin{equation*}\n",
        "y = \\begin{cases}\n",
        "1\\ if\\ \\geq \\theta \\\\\n",
        "0\\ if\\ \\lt \\theta\n",
        "\\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "OR\n",
        "\n",
        "\\begin{equation*}\n",
        "y = \\begin{cases}\n",
        "1\\ if\\ \\geq \\theta \\\\\n",
        "-1\\ if\\ \\lt \\theta\n",
        "\\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "Where theta ($\\theta$) is the threshold.\n",
        "\n",
        "Remember that we pulled `self.a_label` and `self.b_label` from the data in the `__init__()` method and set the threshold (`self.threshold`) to halfway between the two by using the `np.mean()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-tQKo-r5dvd"
      },
      "source": [
        "def predict(self, features):\n",
        "    \"\"\"\n",
        "    \"Predict\" the estimated category as a continuous float.\n",
        "    \"\"\"\n",
        "    # Get the float predicted label and theshold it to make this binary\n",
        "    return self.b_label if self.get_predicted_float(features) < self.threshold else self.a_label\n",
        "\n",
        "Perceptron.predict = predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQWRej4Y2Hgg"
      },
      "source": [
        "### update_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAdc4SNECT17"
      },
      "source": [
        "This method updates the weights in the direction expected to minimize the error by the calculated \"error_amount\" and the \"learning_rate\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X12FRXz_WeLu"
      },
      "source": [
        "def update_weights(self,error_amount, features):\n",
        "    if error_amount == 0.0:\n",
        "        return False\n",
        "    # Now \"nudge\" the weights by the error amount multiplied\n",
        "    # by the learning rate for each feature.\n",
        "    self.weights +=  self.learning_rate * error_amount * features\n",
        "    # Store a copy for later visualization\n",
        "    self.weights_history.append(self.weights.copy())\n",
        "    return True\n",
        "\n",
        "Perceptron.update_weights = update_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXCrYCb2Lng"
      },
      "source": [
        "### train_once()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrfDQNdg5dvi"
      },
      "source": [
        "This is the method that does one iteration over the training samples and tries to improve the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeTl1dVy5dvj"
      },
      "source": [
        "def train_once(self):\n",
        "    # Number of errors encountered (misclassifications)\n",
        "    error_count = 0\n",
        "    # Iterate over each sample in the training set\n",
        "    for features, correct_label in zip(self.samples, self.labels):\n",
        "\n",
        "        # Based on the current weights and bias, what would we predict\n",
        "        # this sample to be?\n",
        "        predicted_label = self.predict(features)\n",
        "        # The difference between the correct and predicted\n",
        "        # tells us how far off we were. This could be positive\n",
        "        # or negative. Note that which you subtract from which is\n",
        "        # very important since the sign matters.\n",
        "        error_amount = predicted_label - correct_label\n",
        "\n",
        "        # If there was a difference, nudge the weights.\n",
        "        if self.update_weights(error_amount,features):\n",
        "            # Since we were wrong, count the error\n",
        "            error_count += 1\n",
        "            pass\n",
        "        pass\n",
        "    return error_count\n",
        "\n",
        "Perceptron.train_once = train_once"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQME46Sk2Shg"
      },
      "source": [
        "### train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k8Jrlpp5dvn"
      },
      "source": [
        "Here is the trian function. The learning rate is sometimes called `eta` after the greek letter $\\eta$ which is used in the math discussions of gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVRXvTPJ5dvn"
      },
      "source": [
        "def train(self, learning_rate=0.01, num_iterations=50, weight_values=None):\n",
        "    # We store the learning rate so that we can view it later\n",
        "    # otherwise it disappears after this method if finished.\n",
        "    self.learning_rate = learning_rate\n",
        "    # Same for num_iterations\n",
        "    self.num_iterations = num_iterations\n",
        "    # Set iteration_count to 0, since we're starting fresh\n",
        "    self.iteration_count = 0\n",
        "    # Initialize biases\n",
        "    self.initialize_weights(weight_values)\n",
        "    self.misclassifications = []\n",
        "    for _ in range(self.num_iterations):\n",
        "        self.iteration_count += 1\n",
        "        misclassifications = self.train_once()\n",
        "        self.misclassifications.append(misclassifications)\n",
        "        if misclassifications <= 0.0:\n",
        "            return self\n",
        "        pass\n",
        "    return self\n",
        " \n",
        "Perceptron.train = train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0iGp4z_D0wx"
      },
      "source": [
        "# Display Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQl4wTnlY7Id"
      },
      "source": [
        "\n",
        "Below we add some display methods that allow us to \"look\" at what is going on in the Perceptron. We do this using a trick that allows us to add methods to a class after the class definition. This is normally not the way you would go about it, but I wanted to have the class definition block only contain the essential class definitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h33-Veh8O0TV"
      },
      "source": [
        "## Print Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cge8J2kH5du_"
      },
      "source": [
        "Here we just add a convenience function that allows us to \"see\" the current Perceptron settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZi3n3PH5dvA"
      },
      "source": [
        "def print_info(self):\n",
        "    print(\"Perceptron:\")\n",
        "    print(\"  Samples: ................ %d\" % self.samples.shape[0])\n",
        "    print(\"  Features per Sample ... : %d\" % (self.samples.shape[1] - 1))\n",
        "    print(\"  Labels: ................. %s\" % self.unique_labels)\n",
        "    print(\"  Threshold: .............. %s\" % self.threshold)\n",
        "    print(\"  Learning Rate (eta): .... %s\" % self.learning_rate)\n",
        "    print(\"  Number of Iterations: ... %s\" % self.num_iterations)\n",
        "    print(\"  Original Bias: .......... %s\" % self.weights_history[0][0])\n",
        "    print(\"  Original Weights: ....... %s\" % self.weights_history[0][1:])\n",
        "    print(\"  Current Bias: ........... %s\" % self.weights[0])\n",
        "    print(\"  Current Weights: ........ %s\" % self.weights[1:])\n",
        "    print(\"  Iterations Completed: ... %s\" % self.iteration_count)\n",
        "    if self.misclassifications is None:\n",
        "        print(\"  Misclassifications: ..... %s\" % self.misclassifications)\n",
        "    else:\n",
        "        print(\"  Misclassifications: ..... %s\" % np.sum(self.misclassifications))\n",
        "        print(\"  Res: %s\" % self.misclassifications)\n",
        "        pass\n",
        "    return\n",
        "\n",
        "Perceptron.print_info = print_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEGhPPOiY9Mn"
      },
      "source": [
        "## Drawing the Decision Boundary Line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrJBEaB9sQVE"
      },
      "source": [
        "To draw a line from one end of the plot to the other we just need to calculate $y$ when $x$ is the smallest value visible on the x-axis ($x_{min}$) and when it is the maximum value on the x-axis ($x_{max}$). We'll call these $y_{x_{min}}$ and $y_{x_{max}}$ respectivly. These equations give us what we want:\n",
        "\n",
        "\\begin{equation}\n",
        "y_{x_{min}} = \\frac{-w_1x_{min}}{w_2}+\\frac{-w_0}{w_2}=\\frac{-(w_1x_{min}+w_0)}{w_2} \\qquad  \\text{and} \\qquad  y_{x_{max}} = \\frac{-w_1x_{max}}{w_2}+\\frac{-w_0}{w_2}=\\frac{-(w_1x_{max}+w_0)}{w_2}\n",
        "\\end{equation}\n",
        "\n",
        "We then just need to draw a line from the point $\\left(x_{min},y_{x_{min}}\\right)$ to $\\left(x_{min},y_{x_{min}}\\right)$ and that's the decision boundary based on the curren weights and bias.\n",
        "\n",
        "**Note**: if $w_2=0$ we have a problem. Also, this assumes we found a boundary. If we did not, the last set of weights my be way off the pot at $x_{min}$ and $x_{max}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C52Wbnx4wcxr"
      },
      "source": [
        "### Detailed Derivation of the Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMV1An2FZCW2"
      },
      "source": [
        "Hopefully you all recall from back in algebra the standard equation for a line:\n",
        "\n",
        "\\begin{equation*}\n",
        "ax + by = c\\  \\Longleftrightarrow\\  ax + by -c = 0\n",
        "\\end{equation*}\n",
        "\n",
        "While the slope form of the line $y = mx + b$ is just this rearranged\n",
        "\n",
        "\\begin{equation*}\n",
        "ax + by = c \\ \\Longleftrightarrow\\  by = -ax + c \\ \\Longleftrightarrow\\  y = \\left( \\frac{-a}{b} \\right) x + c\n",
        "\\end{equation*}\n",
        "\n",
        "Where $\\left( \\frac{-a}{b} \\right)$ corresponds to $m$ (the slope, aka \"rise over run\") and $c$ to $b$.\n",
        "\n",
        "Well, we can use that to draw the boundary line. When we're looking at how we \"predict\" a label, we do so by using this equation from above:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sum_{n=0}^n x_n w_n = ( x_0 w_0 + x_1 w_1 + ... x_n w_n ) = \\begin{bmatrix}\n",
        "x_0 \\\\\n",
        "x_1 \\\\\n",
        "... \\\\\n",
        "x_n\n",
        "\\end{bmatrix}\n",
        "\\bullet\n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "... \\\\\n",
        "w_n\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "\n",
        "But since the $x_0$ value is always `1`, this reduces to:\n",
        "\n",
        "\\begin{equation*}\n",
        "w_0 + ( w_1 \\times x_1 ) + ( w_2 \\times x_2 )\n",
        "\\end{equation*}\n",
        "\n",
        "> or, more simply put\n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "x_1 w_1 + x_2 w_2 + w_0\n",
        "\\end{equation*}\n",
        "\n",
        "Remeber that we plot the features $x_1$ on the x-axis and $x_2$ on the y-axis. We can see that they are just like $x$ and $y$ respectively from the $ax + by = c$ equation. In fact\n",
        "\n",
        "\\begin{equation*}\n",
        "x_1 w_1 + x_2 w_2 + w_0\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Now looks exactly like the $ax + by -c = 0$ equation. By setting the above equation to zero, we can solve for all parts of the line equation. Remember that\n",
        "\n",
        "\\begin{equation}\n",
        "y = x_2 \\ ;\\  x = x_1 \\ ;\\  a = w_1 \\ ;\\  b = w_2 \\ ; \\ c = -w_0 \\\\\n",
        "x_1 w_1 + x_2 w_2 + w_0 = 0\n",
        "\\end{equation}\n",
        "\n",
        "The $y$ intercept is defined as the value of $x$ when $y=0$ (it's where the line intercepts the $y$ axis) and conversely the $x$ intercept is the value of $y$ when $x=0$. Rearranginge the standard equation to solve for $x$ and $y$ individually we have:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "&x = \\frac{c - by}{a} \\ &\\to \\text{when}\\ y=0 &\\to &x = \\frac{c}{a} \\ &\\to \\text{substituting} &\\to &x_1=\\frac{-w_0}{w_1} \\\\\n",
        "&y = \\frac{c - ax}{b} &\\to \\text{when}\\ x=0 &\\to &y = \\frac{c}{b} \\ &\\to \\text{substituting} &\\to &x_2=\\frac{-w_0}{w_2}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "> To calculate the slope\n",
        "\n",
        "\\begin{equation}\n",
        "m=\\frac{-a}{b} \\ \\to \\text{substituting} \\to\\ m = \\frac{-w_1}{w_2} \n",
        "\\end{equation}\n",
        "\n",
        "> plugging values in to get $y$ for the maximum and minimum displayed values of $x$:\n",
        "\n",
        "\\begin{equation}\n",
        "y_{x_{min}}=\\frac{c-ax_{min}}{b} \\ \\to \\text{substituting} \\to\\  y_{x_{min}} = \\frac{-w_o-w_1x_{min}}{w_2} \\\\\n",
        "y_{x_{max}}=\\frac{c-ax_{max}}{b} \\ \\to \\text{substituting} \\to\\  y_{x_{max}} = \\frac{-w_o-w_1x_{max}}{w_2} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "> or more simply using the $y = mx + b$  where $m = \\frac{-w_1}{w_2}$ and $b$ is the y-intercept $b=x_{y=0}=\\frac{-w_0}{w_2}$version of the equation for a line:\n",
        "\n",
        "\\begin{equation}\n",
        "y_{x_{min}}=mx_{min} + b \\ \\to \\text{substituting} \\to\\  y_{x_{min}} = \\frac{-w_1x_{min}}{w_2}+\\frac{-w_0}{w_2} \\\\\n",
        "y_{x_{max}}=mx_{max} + b \\ \\to \\text{substituting} \\to\\  y_{x_{max}} = \\frac{-w_1x_{max}}{w_2}+\\frac{-w_0}{w_2}\n",
        "\\end{equation}\n",
        "\n",
        "As expected, the two are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeHQs1xT5lAE"
      },
      "source": [
        "### get_slope() - Get the slope for the given weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZkzOSQ5tQT"
      },
      "source": [
        "def get_slope(self,weights=None):\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "        pass\n",
        "    return - (weights[1] / weights[2])\n",
        "\n",
        "Perceptron.get_slope = get_slope"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBjiah2kwiPn"
      },
      "source": [
        "### get_y_at_x() - Get a $y$ value for a given $x$ value with the current or provided weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-7rAXJd6FL5"
      },
      "source": [
        "def get_y_at_x(self,x,weights=None):\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "        pass\n",
        "    return -(x * (weights[1] / weights[2])) - (weights[0]/weights[2])\n",
        "\n",
        "Perceptron.get_y_at_x = get_y_at_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBXyzc16FZj"
      },
      "source": [
        "### get_x_at_y() - Get a $x$ value for a given $y$ value with the current or provided weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkLo28oY6NAc"
      },
      "source": [
        "def get_x_at_y(self,y,weights=None):\n",
        "    if weights is None:\n",
        "        weights = self.weights\n",
        "        pass\n",
        "    return -(y * (weights[2] / weights[1])) - (weights[0]/weights[1])\n",
        "\n",
        "Perceptron.get_x_at_y = get_x_at_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZkMusxp6RBA"
      },
      "source": [
        "### plot_boundary_line() - Plot the boundary line and shade the regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_KnMMnz5dv0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_boundary_line(self,ax):\n",
        "    \"\"\"\n",
        "    Calculate the y values for the boundary line at x1 and x2 and draw it on the provided canvas\n",
        "    \"\"\"\n",
        "    # And we can get the first and last x values from the canvas\n",
        "    x_min, x_max = ax.get_xlim()\n",
        "    # get y1 at x_max\n",
        "    y1 = self.get_y_at_x(x_min)\n",
        "    # get y1 at x_min\n",
        "    y2 = self.get_y_at_x(x_max)\n",
        "    #print(\"slope=%0.3f\" % self.get_slope(),\"(x1,y1)=(%0.3f,%0.3f)\"%(x_min,y1),\"(x2,y2)=(%0.3f,%0.3f)\"%(x_max,y2))\n",
        "    # Plotting a line from x_min, y_at_x_min to x_max, y_at_x_max will\n",
        "    # draw the line from one end of the plot to the other that shows\n",
        "    # the decision boundary at the time that this method was called.\n",
        "    ax.plot([x_min,x_max],[y1,y2],'k-',label='decision boundary', alpha=0.5)\n",
        "    # Shade the regions\n",
        "    # Get the min and max values on the y-axis, since that's all that\n",
        "    # is displayed\n",
        "    y_min,y_max = ax.get_ylim()\n",
        "    # Between the line and y_min, color it reddish\n",
        "    ax.fill_between([x_min,x_max],y_min,[y1,y2],facecolor='r', alpha= 0.1)\n",
        "    # Between the line and y_max color it blueish\n",
        "    ax.fill_between([x_min,x_max],y_max,[y1,y2],facecolor='b', alpha= 0.1)\n",
        "    # Add a title\n",
        "    ax.set_title(\"Iris Data with Decision Boundary\")\n",
        "    return\n",
        "\n",
        "Perceptron.plot_boundary_line = plot_boundary_line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL-dQBrLIBBc"
      },
      "source": [
        "## Plotting the Data with the Decision Boundary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmh43zvz5dv2"
      },
      "source": [
        "def plot_data(self,ax):\n",
        "    # Remove the first column (bias values of 1.0)\n",
        "    samples = self.samples[:,1:3]\n",
        "    # Calculate the max and min making them 0.1 wider for the plot so the\n",
        "    # markers fit visibly.\n",
        "    x_min, x_max = samples[:, 0].min() - 0.1, samples[:, 0].max() + 0.1\n",
        "    y_min, y_max = samples[:, 1].min() - 0.1, samples[:, 1].max() + 0.1\n",
        "    # Size plot to fit data tightly\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    # Setosa Samples\n",
        "    setosa = samples[self.labels == self.a_label]\n",
        "    x_s,y_s = setosa[:,0],setosa[:,1]\n",
        "    # Versicolor Samples\n",
        "    versicolor = samples[self.labels == self.b_label]\n",
        "    x_v,y_v = versicolor[:,0],versicolor[:,1]\n",
        "    # Draw the dicision boundry\n",
        "    self.plot_boundary_line(ax)\n",
        "    # Plot the feature samples. We use transparency because some of the samples\n",
        "    # have identical samples and it's nice to see when there are two or more\n",
        "    # \"dots\" on the plot.\n",
        "    ax.scatter(x_s, y_s, color='red', marker='o', label='setosa', alpha=0.33)\n",
        "    ax.scatter(x_v, y_v, color='blue', marker='p', label='versicolor', alpha=0.33)\n",
        "    ax.set_title(\"Iris Data\")\n",
        "    ax.set_xlabel('petal length')\n",
        "    ax.set_ylabel('sepal length')\n",
        "    ax.legend(loc='upper left')\n",
        "    return\n",
        "\n",
        "Perceptron.plot_data = plot_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHQy6vpYIHqn"
      },
      "source": [
        "##Plotting the Errors (Misclassifications) for each Iteration (Epoch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKHvIkWi5dv4"
      },
      "source": [
        "def plot_errors(self,ax, bar=True):\n",
        "    # Titles are nice.\n",
        "    ax.set_title(\"Errors per Iteration (Epoch)\")\n",
        "    ax.set_xlabel('Iterations (Epochs)')\n",
        "    ax.set_ylabel('Number of Errors')\n",
        "    if bar:\n",
        "        x = [x + 1 for x in range(len(self.misclassifications))]\n",
        "        y = self.misclassifications\n",
        "        ax.bar(x,y,color=(0.2, 0.4, 0.6, 0.6))\n",
        "        ax.set_xticklabels(x)\n",
        "        ax.set_xticks(x)\n",
        "        return\n",
        "    # Set the x axis limits\n",
        "    ax.set_xlim(0, len(self.misclassifications))\n",
        "    # Set the y axis limites to one more than then max value so that it\n",
        "    # does not look cut-off\n",
        "    ax.set_ylim(0, np.max(self.misclassifications) + 1)\n",
        "    # We really only want integer tick marks\n",
        "    ax.set_yticks(range(np.max(self.misclassifications) + 1))\n",
        "    ax.set_xticks(range(len(self.misclassifications)))\n",
        "    ax.plot(range(1, len(self.misclassifications) + 1), self.misclassifications, marker='o')\n",
        "    return \n",
        "\n",
        "# Add to classs\n",
        "Perceptron.plot_errors = plot_errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApLYaPVxN2Cg"
      },
      "source": [
        "##Put Table of Attributes on Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r78c5K7T5JsN"
      },
      "source": [
        "### plot_info_table()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSF2EeZA5dv8"
      },
      "source": [
        "def plot_info_table(self,ax):\n",
        "    cellText = []\n",
        "    cellText.append([\"Samples\", \"%d\" % self.samples.shape[0]])\n",
        "    cellText.append([\"Setosa\", \"%d\" % np.sum(self.labels==self.a_label)])\n",
        "    cellText.append([\"Versicolor\", \"%d\" % np.sum(self.labels==self.b_label)])\n",
        "    cellText.append([\"Features\", \"%d\" % (self.samples.shape[1] - 1)])\n",
        "    cellText.append([\"Labels\", \"%s\" % self.unique_labels.shape[0]])\n",
        "    for i,l in enumerate(self.unique_labels):\n",
        "        cellText.append([\" - Label %d\" %(i+1), \"%0.1f\" %l])\n",
        "        pass\n",
        "    cellText.append([\"Threshold\", \"%0.2f\" % self.threshold])\n",
        "    cellText.append([\"Learning Rate ($\\eta$)\", \"%0.2f\" % self.learning_rate])\n",
        "    cellText.append([\"Max Iterations\", \"%s\" % self.num_iterations])\n",
        "    for i,w in enumerate(self.weights_history[0]):\n",
        "        cellText.append([r\"$w_{}$ at $t_0$\".format(i), \"%0.2f\" % w])\n",
        "        pass\n",
        "    t=len(self.misclassifications)\n",
        "    for i,w in enumerate(self.weights):\n",
        "        cellText.append([r\"$w_{}$ at $t_{{{}}}$\".format(i,t), \"%0.2f\" % w])\n",
        "        pass\n",
        "    cellText.append([\"Slope\", \"%0.2f\" % self.get_slope()])\n",
        "    cellText.append([\"Iterations\", \"%s\" % self.iteration_count])\n",
        "    cellText.append([\"Total Errors\", \"%s\" % np.sum(self.misclassifications)])\n",
        "    header_color = (0.4, 0.6, 0.8, 0.25)\n",
        "    table = ax.table(\n",
        "        cellText=cellText,\n",
        "        colLabels=[\"Attribute\",\"Value\"],\n",
        "        colColours=[header_color,header_color],\n",
        "        colWidths=[0.5,0.15],\n",
        "        cellLoc='left',\n",
        "        loc='center right',\n",
        "    )\n",
        "    self.align_col(table,1,\"right\")\n",
        "    table.set_fontsize(12)\n",
        "    table.scale(1.24,1.75)\n",
        "    ax.axis(\"off\")\n",
        "    pass\n",
        "\n",
        "Perceptron.plot_info_table = plot_info_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsFOGovuNv_C"
      },
      "source": [
        "### align_col() - Change the alignment of a matplotlib table column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCHygEDJ4y52"
      },
      "source": [
        "Could not find a better way to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxET02y75dv6"
      },
      "source": [
        "def align_col(self,table, col, align=\"left\"):\n",
        "    cells = [key for key in table._cells if key[1] == col]\n",
        "    for cell in cells:\n",
        "        table._cells[cell]._loc = align\n",
        "        pass\n",
        "    pass\n",
        "\n",
        "Perceptron.align_col = align_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgnaYSYl27dl"
      },
      "source": [
        "##Plotting All Decision Boundaries We Tried"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrspYnQU3D7C"
      },
      "source": [
        "### fit_vals() - Make sure that all the lines fit nicely in a plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXwzeEbO3Gy_"
      },
      "source": [
        "Fit all the attempts on a plot by making all lines go from one side of the plot box to the other, whereever that might be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Ehe8DtRqd6"
      },
      "source": [
        "def fit_vals(self,y0,x_min,x_max,y_min,y_max,ws):\n",
        "    x = self.get_x_at_y(y0,ws)\n",
        "    if x > x_max:\n",
        "        x = self.get_x_at_y(self.get_y_at_x(x_max,ws),ws)\n",
        "    elif x < x_min:\n",
        "        x = self.get_x_at_y(self.get_y_at_x(x_min,ws),ws)\n",
        "        pass\n",
        "    y = self.get_y_at_x(x,ws)\n",
        "    if y > y_max:\n",
        "        y = self.get_y_at_x(self.get_x_at_y(y_max,ws),ws)\n",
        "    elif y < y_min:\n",
        "        y = self.get_y_at_x(self.get_x_at_y(y_min,ws),ws)\n",
        "        pass\n",
        "    return x,y\n",
        "\n",
        "Perceptron.fit_vals = fit_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nawDaPtm3Xvf"
      },
      "source": [
        "### plot_attempts() - Make a plot with all the decision boundaries we tried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slX7d7UM3dfC"
      },
      "source": [
        "Create a plot with all the decision boundaries we've tried.\n",
        "\n",
        "**Notes**:\n",
        "\n",
        "- we only keep track of changes to the weights, not the number of times those weights were used, so the number of lines will almost never match the number of iterations or even the number of iterations times the number of samples.\n",
        "- we dont (yet) handle vertical lines (where $w_2$ = 0) since in the quick-and-dirty methods we have we divide by $w_2$ to get our values. Some day, I may change that, but this is an issue if you set all weights to `0.0`, which we do. In that case, we just ignore the `NaN` errors but Python will complain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr4gwXyF3YTK"
      },
      "source": [
        "def plot_attempts(self,ax):\n",
        "    # get all the values of y where x = 0\n",
        "    y_int = np.array([self.get_y_at_x(0,weights) for weights in self.weights_history])\n",
        "    # get all the values of x where y = 0\n",
        "    x_int = np.array([self.get_x_at_y(0,weights) for weights in self.weights_history])\n",
        "    # remove any pairs with NaN values\n",
        "    idxs = []\n",
        "    wsh = []\n",
        "    for i, y in enumerate(y_int):\n",
        "        if np.isnan(y) or np.isnan(x_int[i]):\n",
        "            idxs.append(False)\n",
        "        else:\n",
        "            idxs.append(True)\n",
        "            wsh.append(self.weights_history[i])\n",
        "            pass\n",
        "        pass\n",
        "    y_int = y_int[idxs]\n",
        "    x_int = x_int[idxs]\n",
        "    # Find the min and max for x and y axes\n",
        "    y_min, y_max, x_min, x_max = y_int.min(), y_int.max(), x_int.min(), x_int.max()\n",
        "    # Shrink canvas\n",
        "    ax.set_xlim(x_min,x_max)\n",
        "    ax.set_ylim(y_min,y_max)\n",
        "    # Now loop through all the non-NaN weights and draw the longest line you can\n",
        "    for ws in wsh:\n",
        "        x1,y1 = self.fit_vals(y_min,x_min,x_max,y_min,y_max,ws)\n",
        "        x2,y2 = self.fit_vals(y_max,x_min,x_max,y_min,y_max,ws)\n",
        "        ax.plot([x1,x2],[y1,y2],'k-', alpha=0.33)\n",
        "        pass\n",
        "    ax.set_title(\"Boundaries Tried\")\n",
        "    return\n",
        "\n",
        "Perceptron.plot_attempts = plot_attempts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkFhmKhyOGqF"
      },
      "source": [
        "## Create Comprehensive Information Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhpxiHo84-aV"
      },
      "source": [
        "### into() - Plot the conprehensive visualization of the Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFJEMOxz5dv-"
      },
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "def info(self, save=False, show=True):\n",
        "    plt.close()\n",
        "    # Create three subplots in a 4 x 3 grid\n",
        "    grid = (4,3)\n",
        "    # Create a decent sized figure\n",
        "    fig = plt.figure(figsize=(12, 12), dpi=96, facecolor=None)\n",
        "    # Top left for misclassifications per epoc\n",
        "    err_ax = plt.subplot2grid(grid,(0,0), colspan=2)\n",
        "    self.plot_errors(err_ax)\n",
        "    # Top Right for text info\n",
        "    inf_ax = plt.subplot2grid(grid,(0,2), rowspan=4)\n",
        "    self.plot_info_table(inf_ax)\n",
        "    # Middle for data and boundary\n",
        "    dat_ax = plt.subplot2grid(grid, (1,0), colspan=2, rowspan=2)\n",
        "    self.plot_data(dat_ax)\n",
        "    # Bottom for attempts\n",
        "    att_ax = plt.subplot2grid(grid, (3,0), colspan=2)\n",
        "    self.plot_attempts(att_ax)\n",
        "    fig.tight_layout()\n",
        "    if save:\n",
        "        import time\n",
        "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        filename = \"Perceptron-%s.svg\" % timestr\n",
        "        print(\"Saving '%s'\" %(filename))\n",
        "        plt.savefig(filename)\n",
        "        pass\n",
        "    if show:\n",
        "        plt.show()\n",
        "        pass\n",
        "    return self\n",
        "\n",
        "# Add to class\n",
        "Perceptron.info = info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdB8FcA7UhYa"
      },
      "source": [
        "# Running the Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CqICAR1_rNm"
      },
      "source": [
        "# Hopefully it looks good at this point. So let's run a trial and plot the\n",
        "# misclassifications each iteration (epoch)\n",
        "perc = Perceptron(samples,labels)\n",
        "_ = perc.train(learning_rate=0.01, num_iterations=50, weight_values=0.0).info(save=True,show=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF2fvH2wA3AE"
      },
      "source": [
        "# Now try with random weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJEQc-Qp5dwH"
      },
      "source": [
        "# Now try with random weights & changing the learning rate = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jykep6EHsSWd"
      },
      "source": [
        "# Now try with random weights & learning rate = 0.1 & changing the num_iterations to 3 \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}